{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to submit OpenAI GPT-3.5-turbo Fine-tuning Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook file, I will go through how to submit a GPT-3.5-turbo fine-tuning job to OpenAI. Note that as of Sept 1st 2023, <br>\n",
    "the fine-tuning feature is only available for GPT-3.5-turbo, with the feature expected to be released for GPT-4 in Fall of the same year.\n",
    "\n",
    "In this document, I will go through the following:\n",
    "- Data Preparation\n",
    "- Error Checking\n",
    "- Training File Upload\n",
    "- Job Submission\n",
    "- Status Tracking\n",
    "- Calling a Fine-tuned Model\n",
    "\n",
    "References:\n",
    "- https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates\n",
    "- https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have to submit a .jsonl training file for the fine-tuning job. Each sample should look like a flattened version of this:\n",
    "\n",
    "Each sample should include the following:\n",
    "- a system message: a pre-prompt that tells GPT what it is supposed to mimic.\n",
    "- a user message: the main prompt including the instructions to execute a task.\n",
    "- an assistant message: what you want GPT to respond with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"messages\": [\n",
    "    { \"role\": \"system\", \"content\": \"You are an assistant that occasionally misspells words\" },\n",
    "    { \"role\": \"user\", \"content\": \"Tell me a story.\" },\n",
    "    { \"role\": \"assistant\", \"content\": \"One day a student went to schoool.\" }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, if you have a .json file of training samples formatted as above, you can simply convert it to a .jsonl file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Read the original JSON file\n",
    "with open('training_file.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Write each object as a separate JSON object in a JSONL file\n",
    "with open('training_file.jsonl', 'w') as f:\n",
    "    for obj in data:\n",
    "        json_line = json.dumps(obj)\n",
    "        f.write(json_line + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Error Checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes use of the code from the OpenAI fine-tuning documentation. This is to make sure your .jsonl file is in the correct format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by importing the required packages\n",
    "\n",
    "import json\n",
    "import os\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we specify the data path and open the JSONL file\n",
    "\n",
    "data_path = \"training_file.jsonl\"\n",
    "\n",
    "# Load dataset\n",
    "with open(data_path) as f:\n",
    "    dataset = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 305\n",
      "First example:\n",
      "{'role': 'system', 'content': 'You are an assistant that generate answerable user-like questions from a passage.'}\n",
      "{'role': 'user', 'content': 'Given a passage from the user manual of a specific machinery, imagine you are operating said machinery, generate a set of questions that a user operating it might ask.\\n\\nPassage:\\nTo disable and enable Cruise Grade Braking for the current ignition cycle, press and hold the Tow/Haul button for five seconds. A Driver Information Center (DIC) message displays.\\n\\nEach question should be user-like.\\nEach question should be closely related to the passage.\\nEach question must be answerable by the information in the passage.\\nThe set of questions should be sufficient to completely cover the information in the passage.\\nQuestions must not be questions about the passage location.\\nEach question should not be a combination of two or more questions.\\nEach question should focus on practical application, problem-solving, or inquiry for clarification/definition, rather than being a quiz show question.\\nEach question should be clear and brief.\\nEach question should not be overly-specific.'}\n",
      "{'role': 'assistant', 'content': 'How do I know whether the cruise grade braking is engaged?\\nHow do I know whether the cruise grade braking is not engaged?\\nHow do I disable cruise grade braking for the current ignition cycle?\\nHow do I enable cruise grade braking for the current ignition cycle?'}\n"
     ]
    }
   ],
   "source": [
    "# We can inspect the data quickly by checking the number of examples and the first item\n",
    "\n",
    "# Initial dataset stats\n",
    "print(\"Num examples:\", len(dataset))\n",
    "print(\"First example:\")\n",
    "for message in dataset[0][\"messages\"]:\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a sense of the data, we need to go through all the different examples and check to make sure the formatting is correct and matches the Chat completions message structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors found\n"
     ]
    }
   ],
   "source": [
    "# Format error checks\n",
    "format_errors = defaultdict(int)\n",
    "\n",
    "for ex in dataset:\n",
    "    if not isinstance(ex, dict):\n",
    "        format_errors[\"data_type\"] += 1\n",
    "        continue\n",
    "\n",
    "    messages = ex.get(\"messages\", None)\n",
    "    if not messages:\n",
    "        format_errors[\"missing_messages_list\"] += 1\n",
    "        continue\n",
    "\n",
    "    for message in messages:\n",
    "        if \"role\" not in message or \"content\" not in message:\n",
    "            format_errors[\"message_missing_key\"] += 1\n",
    "\n",
    "        if any(k not in (\"role\", \"content\", \"name\") for k in message):\n",
    "            format_errors[\"message_unrecognized_key\"] += 1\n",
    "\n",
    "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\"):\n",
    "            format_errors[\"unrecognized_role\"] += 1\n",
    "\n",
    "        content = message.get(\"content\", None)\n",
    "        if not content or not isinstance(content, str):\n",
    "            format_errors[\"missing_content\"] += 1\n",
    "\n",
    "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "        format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "if format_errors:\n",
    "    print(\"Found errors:\")\n",
    "    for k, v in format_errors.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"No errors found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond the structure of the message, we also need to ensure that the length does not exceed the 4096 token limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token counting functions\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# not exact!\n",
    "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n",
    "\n",
    "def num_assistant_tokens_from_messages(messages):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
    "    return num_tokens\n",
    "\n",
    "def print_distribution(values, name):\n",
    "    print(f\"\\n#### Distribution of {name}:\")\n",
    "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
    "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples missing system message: 0\n",
      "Num examples missing user message: 0\n",
      "\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 3, 3\n",
      "mean / median: 3.0, 3.0\n",
      "p5 / p95: 3.0, 3.0\n",
      "\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 212, 700\n",
      "mean / median: 319.11803278688524, 308.0\n",
      "p5 / p95: 248.4, 395.20000000000005\n",
      "\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 26, 437\n",
      "mean / median: 68.85573770491803, 57.0\n",
      "p5 / p95: 39.0, 105.80000000000007\n",
      "\n",
      "0 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n",
      "Dataset has ~97331 tokens that will be charged for during training\n",
      "By default, you'll train for 3 epochs on this dataset\n",
      "By default, you'll be charged for ~291993 tokens\n",
      "See pricing page to estimate total costs\n"
     ]
    }
   ],
   "source": [
    "# Warnings and tokens counts\n",
    "n_missing_system = 0\n",
    "n_missing_user = 0\n",
    "n_messages = []\n",
    "convo_lens = []\n",
    "assistant_message_lens = []\n",
    "\n",
    "for ex in dataset:\n",
    "    messages = ex[\"messages\"]\n",
    "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "        n_missing_system += 1\n",
    "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "        n_missing_user += 1\n",
    "    n_messages.append(len(messages))\n",
    "    convo_lens.append(num_tokens_from_messages(messages))\n",
    "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "\n",
    "print(\"Num examples missing system message:\", n_missing_system)\n",
    "print(\"Num examples missing user message:\", n_missing_user)\n",
    "print_distribution(n_messages, \"num_messages_per_example\")\n",
    "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "n_too_long = sum(l > 4096 for l in convo_lens)\n",
    "print(f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\")\n",
    "\n",
    "# Pricing and default n_epochs estimate\n",
    "MAX_TOKENS_PER_EXAMPLE = 4096\n",
    "\n",
    "MIN_TARGET_EXAMPLES = 100\n",
    "MAX_TARGET_EXAMPLES = 25000\n",
    "TARGET_EPOCHS = 3\n",
    "MIN_EPOCHS = 1\n",
    "MAX_EPOCHS = 25\n",
    "\n",
    "n_epochs = TARGET_EPOCHS\n",
    "n_train_examples = len(dataset)\n",
    "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "    n_epochs = min(MAX_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "    n_epochs = max(MIN_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")\n",
    "print(\"See pricing page to estimate total costs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training File Upload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have checked that the training file is in the correct format, we can start uploading the training file to OpenAI.\n",
    "\n",
    "Make sure you have the latest version of the openai package. (I couldn't find the version require for fine-tuning online, just that I need to update it.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading the training file\n",
    "\n",
    "training_file_name = 'training_file.jsonl'\n",
    "\n",
    "training_response = openai.File.create(\n",
    "    file=open(training_file_name, 'rb'), purpose=\"fine-tune\"\n",
    ")\n",
    "\n",
    "training_file_id = training_response[\"id\"]\n",
    "\n",
    "print(\"Training file id:\", training_file_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Job Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the training file uploaded, we can submit a fine-tuning job. Note that for each organization, there can only be one fine-tuning job running at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suffix name is the name you are giving to the model\n",
    "suffix_name = \"iNAGO-QGen-example\"\n",
    "\n",
    "# fine-tuning job submission\n",
    "response = openai.FineTuningJob.create(\n",
    "    training_file = training_file_id,\n",
    "    model = \"gpt-3.5-turbo\",\n",
    "    suffix = suffix_name\n",
    ")\n",
    "\n",
    "job_id = response[\"id\"]\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Status Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can track the fine-tuning process with the code below. Note that a fine-tuning job usually starts after 10-15 minutes after submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.FineTuningJob.list_events(id=job_id, limit=50)\n",
    "\n",
    "events = response[\"data\"]\n",
    "events.reverse()\n",
    "\n",
    "for event in events:\n",
    "    print(event['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Calling a Fine-tuned Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To call a fine-tuned model, you need to specify the model_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What does the phone need to be powered on to authenticate it?\n",
      "What does the phone need to enable it to use as a key?\n",
      "What happens when my Model 3 is detected by the mobile app?\n",
      "When do I need to tap the key card against the Model 3 card reader on the door pillar?\n",
      "When do I need to tap the key card against the Model 3 card reader on the centre console?\n",
      "What must I do to use the phone to access Model 3?\n",
      "What must I do before I can use a phone to access Model 3?\n",
      "How do I authenticate the phone to use as a key?\n",
      "What should I do to use the phone as a key?\n",
      "How to ensure Model 3 and my phone stay connected?\n"
     ]
    }
   ],
   "source": [
    "# calling a fine-tuned model\n",
    "\n",
    "test_messages = []\n",
    "test_messages.append({\"role\": \"system\", \"content\": \"You are an assistant that generate answerable user-like questions from a passage.\"})\n",
    "prompt = f'''Generate around ten questions from the following passage:\n",
    "\n",
    "Before you can use a phone to access Model 3, follow these steps to authenticate it:\n",
    "Download the Tesla mobile app to your phone. Log into the Tesla mobile app using your Tesla Account user name and password. NOTE: You must remain logged in to your Tesla Account to use your phone to access Model 3.\n",
    "Ensure that your phone's Bluetooth settings are turned on. You must have your phone's Bluetooth setting turned on AND you must also ensure that Bluetooth is turned on within your phone's global settings for the Tesla mobile app.\n",
    "For example, on your phone, navigate to Settings, choose the Tesla mobile app, and ensure the Bluetooth setting is enabled. NOTE: Model 3 communicates with your phone using Bluetooth.\n",
    "To authenticate your phone or use it as a key, the phone must be powered on and Bluetooth must be enabled. Keep in mind that your phone must have enough battery power to run Bluetooth and that many phones disable Bluetooth when the battery is low.\n",
    "Ensure that Allow Mobile Access (Controls > Safety & Security > Allow Mobile Access) is enabled. In the Tesla mobile app, touch PHONE KEY then touch START to search for your Model 3. When your Model 3 is detected, the mobile app asks you to tap your key card.\n",
    "Tap the key card against the Model 3 card reader on the door pillar or center console (see Key Card on page 9).'''\n",
    "test_messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "model_id = os.environ.get(\"MODEL_ID\")\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model = model_id, messages = test_messages, temperature = 1, n = 1\n",
    ")\n",
    "print(response[\"choices\"][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling a fine-tuned model\n",
    "\n",
    "import openai\n",
    "\n",
    "test_messages = []\n",
    "test_messages.append({\"role\": \"system\", \"content\": \"You are an assistant that generate answerable user-like questions from a passage.\"})\n",
    "prompt = '''Your prompt...'''\n",
    "test_messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "model_id = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model = model_id, messages = test_messages, temperature = 1, n = 1\n",
    ")\n",
    "print(response[\"choices\"][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eecs6322-a3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
